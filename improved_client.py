#!/usr/bin/env python3
"""
ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê°œì„ ëœ FedHybrid í´ë¼ì´ì–¸íŠ¸
"""

import torch
import requests
from advanced_model import AdvancedEnhancerModel, advanced_client_update, load_advanced_diabetes_data
from aggregation import CommunicationEfficientFedHB
from torch.utils.data import DataLoader
import torch.nn as nn
import os
import numpy as np
import pandas as pd
import time
from ckks import batch_encrypt, batch_decrypt
import argparse
import sys
import io
import matplotlib.pyplot as plt
import openpyxl
from openpyxl.drawing.image import Image

# device ì„¤ì •
device = torch.device('cpu')

# í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
CLIENT_ID = os.getenv('CLIENT_ID', 'client_1')

# CKKS íŒŒë¼ë¯¸í„° ì„¤ì •
z_q = 1 << 10
rescale_q = z_q
N = 4
s = np.array([1+0j, 1+0j, 0+0j, 0+0j], dtype=np.complex128)

SERVER_URL = "http://localhost:8000"
NUM_ROUNDS = 5

# ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
OPTIMIZED_PARAMS = {
    'hidden_dims': [256, 128, 64],
    'dropout_rate': 0.174,
    'learning_rate': 0.0027,
    'weight_decay': 7.19e-5,
    'batch_size': 64,
    'epochs': 13,
    'activation': 'relu',
    'optimizer': 'adamw',
    'scheduler': 'cosine'
}

def evaluate_local_accuracy(model, data_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for x, y in data_loader:
            x, y = x.to(device), y.to(device)
            outputs = model(x)
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == y).sum().item()
            total += y.size(0)
    acc = correct / total * 100 if total > 0 else 0.0
    return acc

def download_global_model():
    for attempt in range(5):
        try:
            print(f"ê¸€ë¡œë²Œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œë„ {attempt + 1}/5...")
            r = requests.get(f"{SERVER_URL}/get_model", timeout=10)
            
            if r.status_code == 200:
                with open("global_model.pth", "wb") as f:
                    f.write(r.content)
                
                file_size = os.path.getsize("global_model.pth")
                print(f"ê¸€ë¡œë²Œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (íŒŒì¼ í¬ê¸°: {file_size} bytes)")
                
                try:
                    model_data = torch.load("global_model.pth", map_location=device, weights_only=False)
                    
                    if isinstance(model_data, dict) and 'state_dict' in model_data:
                        print(f"ëª¨ë¸ ë©”íƒ€ë°ì´í„°: {model_data.get('model_type', 'Unknown')} v{model_data.get('version', 'Unknown')}")
                        print(f"ì„œë²„ ëª¨ë¸ ì…ë ¥ ì°¨ì›: {model_data.get('input_dim', 'Unknown')}")
                        state_dict = model_data['state_dict']
                    else:
                        print("êµ¬ í˜•ì‹ì˜ ëª¨ë¸ íŒŒì¼ì…ë‹ˆë‹¤.")
                        state_dict = model_data
                    
                    os.remove("global_model.pth")
                    print("ê¸€ë¡œë²Œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                    return state_dict
                except Exception as e:
                    print(f"ê¸€ë¡œë²Œ ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    if os.path.exists("global_model.pth"):
                        os.remove("global_model.pth")
            else:
                print(f"ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {r.status_code} - {r.text}")
                
        except requests.exceptions.ConnectionError:
            print(f"ì„œë²„ ì—°ê²° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/5)")
        except requests.exceptions.Timeout:
            print(f"ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ (ì‹œë„ {attempt + 1}/5)")
        except Exception as e:
            print(f"ê¸€ë¡œë²Œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        if attempt < 4:
            print("3ì´ˆ í›„ ì¬ì‹œë„...")
            time.sleep(3)
    
    raise RuntimeError("ê¸€ë¡œë²Œ ëª¨ë¸ì„ ì •ìƒì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

def analyze_feature_importance(model, data_loader, feature_names, device):
    """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
    model.eval()
    feature_importance = {}
    
    with torch.no_grad():
        for x, y in data_loader:
            x, y = x.to(device), y.to(device)
            x.requires_grad_(True)
            
            outputs = model(x)
            loss = nn.CrossEntropyLoss()(outputs, y)
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ì˜ ì ˆëŒ“ê°’ í‰ê· ìœ¼ë¡œ ì¤‘ìš”ë„ ê³„ì‚°
            gradients = x.grad.abs().mean(dim=0)
            
            for i, feature_name in enumerate(feature_names):
                if feature_name not in feature_importance:
                    feature_importance[feature_name] = []
                feature_importance[feature_name].append(gradients[i].item())
            
            break  # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ ì‚¬ìš©
    
    # í‰ê·  ì¤‘ìš”ë„ ê³„ì‚°
    avg_importance = {}
    for feature_name, values in feature_importance.items():
        avg_importance[feature_name] = np.mean(values)
    
    # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_importance = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)
    
    return sorted_importance

def predict_diabetes_probability_with_explanation(model, data_loader, feature_names, device):
    """ë‹¹ë‡¨ë³‘ í™•ë¥  ì˜ˆì¸¡ ë° ì„¤ëª…"""
    model.eval()
    probabilities = []
    predictions = []
    feature_importance = analyze_feature_importance(model, data_loader, feature_names, device)
    
    with torch.no_grad():
        for x, _ in data_loader:
            x = x.to(device)
            outputs = model(x)
            probs = torch.softmax(outputs, dim=1)
            
            # ë‹¹ë‡¨ë³‘ í™•ë¥  (í´ë˜ìŠ¤ 1)
            diabetes_probs = probs[:, 1].cpu().numpy()
            probabilities.extend(diabetes_probs)
            
            # ì˜ˆì¸¡ í´ë˜ìŠ¤
            _, predicted = torch.max(outputs, 1)
            predictions.extend(predicted.cpu().numpy())
    
    return np.array(probabilities), np.array(predictions), feature_importance

def save_results_to_excel_with_graphs(original_data, probabilities, predictions, feature_importance=None, output_path='prediction_results.xlsx', round_accuracies=None, round_losses=None):
    """ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ (ê·¸ë˜í”„ í¬í•¨)"""
    try:
        print(f"ê²°ê³¼ ì €ì¥ ì‹œì‘: {len(probabilities)}ê°œ ë°ì´í„°")
        
        # NaN ê°’ ì²˜ë¦¬
        probabilities = np.nan_to_num(probabilities, nan=0.0, posinf=1.0, neginf=0.0)
        predictions = np.nan_to_num(predictions, nan=0, posinf=1, neginf=0).astype(int)
        
        # ë°ì´í„° í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ë° ì‹œê°„ ì ˆì•½)
        max_rows = 10000  # ìµœëŒ€ 10,000í–‰ìœ¼ë¡œ ì œí•œ
        if len(original_data) > max_rows:
            print(f"ë°ì´í„° í¬ê¸°ê°€ í½ë‹ˆë‹¤. ìƒìœ„ {max_rows}ê°œ í–‰ë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
            # í™•ë¥  ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ë°ì´í„°ë§Œ ì„ íƒ
            top_indices = np.argsort(probabilities)[-max_rows:]
            original_data = original_data.iloc[top_indices]
            probabilities = probabilities[top_indices]
            predictions = predictions[top_indices]
        
        # ì›ë³¸ ë°ì´í„°ì— ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
        result_df = original_data.copy()
        
        # ë¶ˆí•„ìš”í•œ Unnamed ì»¬ëŸ¼ë“¤ ì œê±°
        unnamed_cols = [col for col in result_df.columns if col.startswith('Unnamed:')]
        if unnamed_cols:
            print(f"ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°: {unnamed_cols}")
            result_df = result_df.drop(columns=unnamed_cols)
        
        result_df['ë‹¹ë‡¨ë³‘_í™•ë¥ '] = probabilities
        result_df['ì˜ˆì¸¡_ê²°ê³¼'] = predictions
        result_df['ì˜ˆì¸¡_ë¼ë²¨'] = ['ë‹¹ë‡¨ë³‘' if p == 1 else 'ì •ìƒ' for p in predictions]
        
        # í™•ë¥ ë³„ë¡œ ì •ë ¬
        result_df = result_df.sort_values('ë‹¹ë‡¨ë³‘_í™•ë¥ ', ascending=False)
        
        print(f"ì—‘ì…€ íŒŒì¼ ì €ì¥ ì‹œì‘: {len(result_df)}í–‰")
        
        # ì—‘ì…€ íŒŒì¼ ìƒì„± (ê·¸ë˜í”„ í¬í•¨)
        try:
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                # ë°ì´í„° ì‹œíŠ¸
                result_df.to_excel(writer, sheet_name='ì˜ˆì¸¡_ê²°ê³¼', index=False)
                
                # í†µê³„ ì‹œíŠ¸
                stats_data = {
                    'í†µê³„': [
                        'ì´ ë°ì´í„° ìˆ˜',
                        'ë‹¹ë‡¨ë³‘ ì˜ˆì¸¡ ìˆ˜',
                        'ì •ìƒ ì˜ˆì¸¡ ìˆ˜',
                        'í‰ê·  ë‹¹ë‡¨ë³‘ í™•ë¥ ',
                        'ìµœëŒ€ ë‹¹ë‡¨ë³‘ í™•ë¥ ',
                        'ìµœì†Œ ë‹¹ë‡¨ë³‘ í™•ë¥ '
                    ],
                    'ê°’': [
                        len(result_df),
                        sum(predictions),
                        len(predictions) - sum(predictions),
                        f"{np.mean(probabilities):.4f}",
                        f"{np.max(probabilities):.4f}",
                        f"{np.min(probabilities):.4f}"
                    ]
                }
                stats_df = pd.DataFrame(stats_data)
                stats_df.to_excel(writer, sheet_name='í†µê³„', index=False)
                
                # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œíŠ¸
                if feature_importance:
                    importance_data = {
                        'íŠ¹ì„±ëª…': [feature for feature, _ in feature_importance],
                        'ì¤‘ìš”ë„': [importance for _, importance in feature_importance]
                    }
                    importance_df = pd.DataFrame(importance_data)
                    importance_df.to_excel(writer, sheet_name='íŠ¹ì„±_ì¤‘ìš”ë„', index=False)
                
                # ë¼ìš´ë“œë³„ ì •í™•ë„ ì‹œíŠ¸
                if round_accuracies and round_losses:
                    round_data = {
                        'ë¼ìš´ë“œ': list(range(1, len(round_accuracies) + 1)),
                        'ì •í™•ë„(%)': round_accuracies,
                        'ì†ì‹¤': round_losses
                    }
                    round_df = pd.DataFrame(round_data)
                    round_df.to_excel(writer, sheet_name='ë¼ìš´ë“œë³„_ì„±ëŠ¥', index=False)
                
                # ì›Œí¬ë¶ ê°€ì ¸ì˜¤ê¸°
                workbook = writer.book
                
                # í™•ë¥  ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
                try:
                    import matplotlib.pyplot as plt
                    plt.figure(figsize=(10, 6))
                    plt.hist(probabilities, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
                    plt.title('ë‹¹ë‡¨ë³‘ í™•ë¥  ë¶„í¬', fontsize=14, fontweight='bold')
                    plt.xlabel('ë‹¹ë‡¨ë³‘ í™•ë¥ ', fontsize=12)
                    plt.ylabel('ë¹ˆë„', fontsize=12)
                    plt.grid(True, alpha=0.3)
                    
                    # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ì €ì¥
                    img_buffer = io.BytesIO()
                    plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                    img_buffer.seek(0)
                    
                    # ì—‘ì…€ì— ì´ë¯¸ì§€ ì¶”ê°€
                    worksheet = writer.sheets['ì˜ˆì¸¡_ê²°ê³¼']
                    img = openpyxl.drawing.image.Image(img_buffer)
                    img.anchor = f'A{len(result_df) + 3}'
                    worksheet.add_image(img)
                    
                    plt.close()
                    print("í™•ë¥  ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ ì¶”ê°€ ì™„ë£Œ")
                except Exception as e:
                    print(f"íˆìŠ¤í† ê·¸ë¨ ìƒì„± ì‹¤íŒ¨: {e}")
                
                # íŠ¹ì„± ì¤‘ìš”ë„ ë§‰ëŒ€ ê·¸ë˜í”„
                if feature_importance:
                    try:
                        plt.figure(figsize=(12, 8))
                        features = [feature for feature, _ in feature_importance[:10]]  # ìƒìœ„ 10ê°œ
                        importances = [importance for _, importance in feature_importance[:10]]
                        
                        plt.barh(range(len(features)), importances, color='lightcoral')
                        plt.yticks(range(len(features)), features)
                        plt.xlabel('ì¤‘ìš”ë„', fontsize=12)
                        plt.title('íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ)', fontsize=14, fontweight='bold')
                        plt.grid(True, alpha=0.3)
                        
                        # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ì €ì¥
                        img_buffer2 = io.BytesIO()
                        plt.savefig(img_buffer2, format='png', dpi=300, bbox_inches='tight')
                        img_buffer2.seek(0)
                        
                        # ì—‘ì…€ì— ì´ë¯¸ì§€ ì¶”ê°€
                        worksheet = writer.sheets['íŠ¹ì„±_ì¤‘ìš”ë„']
                        img2 = openpyxl.drawing.image.Image(img_buffer2)
                        img2.anchor = 'A1'
                        worksheet.add_image(img2)
                        
                        plt.close()
                        print("íŠ¹ì„± ì¤‘ìš”ë„ ê·¸ë˜í”„ ì¶”ê°€ ì™„ë£Œ")
                    except Exception as e:
                        print(f"íŠ¹ì„± ì¤‘ìš”ë„ ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {e}")
                
                # ë¼ìš´ë“œë³„ ì •í™•ë„ ë° ì†ì‹¤ ì°¨íŠ¸
                if round_accuracies and round_losses:
                    try:
                        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
                        
                        # ì •í™•ë„ ì°¨íŠ¸
                        rounds = list(range(1, len(round_accuracies) + 1))
                        ax1.plot(rounds, round_accuracies, 'o-', color='blue', linewidth=2, markersize=6)
                        ax1.set_title('ë¼ìš´ë“œë³„ ì •í™•ë„ ë³€í™”', fontsize=14, fontweight='bold')
                        ax1.set_xlabel('ë¼ìš´ë“œ', fontsize=12)
                        ax1.set_ylabel('ì •í™•ë„ (%)', fontsize=12)
                        ax1.grid(True, alpha=0.3)
                        ax1.set_ylim(0, 100)
                        
                        # ê° ì ì— ê°’ í‘œì‹œ
                        for i, acc in enumerate(round_accuracies):
                            ax1.annotate(f'{acc:.1f}%', (rounds[i], acc), 
                                       textcoords="offset points", xytext=(0,10), 
                                       ha='center', fontsize=10)
                        
                        # ì†ì‹¤ ì°¨íŠ¸
                        ax2.plot(rounds, round_losses, 'o-', color='red', linewidth=2, markersize=6)
                        ax2.set_title('ë¼ìš´ë“œë³„ ì†ì‹¤ ë³€í™”', fontsize=14, fontweight='bold')
                        ax2.set_xlabel('ë¼ìš´ë“œ', fontsize=12)
                        ax2.set_ylabel('ì†ì‹¤', fontsize=12)
                        ax2.grid(True, alpha=0.3)
                        
                        # ê° ì ì— ê°’ í‘œì‹œ
                        for i, loss in enumerate(round_losses):
                            ax2.annotate(f'{loss:.4f}', (rounds[i], loss), 
                                       textcoords="offset points", xytext=(0,10), 
                                       ha='center', fontsize=10)
                        
                        plt.tight_layout()
                        
                        # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ì €ì¥
                        img_buffer3 = io.BytesIO()
                        plt.savefig(img_buffer3, format='png', dpi=300, bbox_inches='tight')
                        img_buffer3.seek(0)
                        
                        # ì—‘ì…€ì— ì´ë¯¸ì§€ ì¶”ê°€
                        worksheet = writer.sheets['ë¼ìš´ë“œë³„_ì„±ëŠ¥']
                        img3 = openpyxl.drawing.image.Image(img_buffer3)
                        img3.anchor = 'A1'
                        worksheet.add_image(img3)
                        
                        plt.close()
                        print("ë¼ìš´ë“œë³„ ì„±ëŠ¥ ì°¨íŠ¸ ì¶”ê°€ ì™„ë£Œ")
                    except Exception as e:
                        print(f"ë¼ìš´ë“œë³„ ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            
            print(f"ì—‘ì…€ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")
            print(f"íŒŒì¼ í¬ê¸°: {os.path.getsize(output_path)} bytes")
            return True
            
        except Exception as excel_error:
            print(f"ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨, CSVë¡œ ëŒ€ì²´ ì €ì¥: {excel_error}")
            csv_path = output_path.replace('.xlsx', '.csv')
            result_df.to_csv(csv_path, index=False)
            print(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {csv_path}")
            return False
        
    except Exception as e:
        print(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    parser = argparse.ArgumentParser(description='FedHybrid í´ë¼ì´ì–¸íŠ¸ (ìµœì í™” ë²„ì „)')
    parser.add_argument('--input_file', type=str, required=True, help='ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ')
    args = parser.parse_args()
    
    data_file = args.input_file
    
    if not os.path.exists(data_file):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_file}")
        return False
    
    print(f"=== FedHybrid í´ë¼ì´ì–¸íŠ¸ (ìµœì í™” ë²„ì „) ===")
    print(f"ğŸ“ ì…ë ¥ íŒŒì¼: {data_file}")
    print(f"ğŸ”§ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©")
    print(f"  - Hidden dims: {OPTIMIZED_PARAMS['hidden_dims']}")
    print(f"  - Dropout rate: {OPTIMIZED_PARAMS['dropout_rate']}")
    print(f"  - Learning rate: {OPTIMIZED_PARAMS['learning_rate']}")
    print(f"  - Batch size: {OPTIMIZED_PARAMS['batch_size']}")
    print(f"  - Epochs: {OPTIMIZED_PARAMS['epochs']}")
    
    # ë°ì´í„° ë¡œë”©
    try:
        train_dataset, test_dataset, input_dim, class_weights, selected_features = load_advanced_diabetes_data(data_file)
        train_loader = DataLoader(train_dataset, batch_size=OPTIMIZED_PARAMS['batch_size'], shuffle=True, num_workers=0)
        test_loader = DataLoader(test_dataset, batch_size=OPTIMIZED_PARAMS['batch_size'], shuffle=False, num_workers=0)
        print(f"âœ… ê³ ê¸‰ ë°ì´í„° ë¡œë“œ ì™„ë£Œ - ì…ë ¥ ì°¨ì›: {input_dim}")
        print(f"ì„ íƒëœ íŠ¹ì„±: {selected_features}")
        print(f"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weights}")
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

    # ëª¨ë¸ ì¤€ë¹„ (ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
    client_model = AdvancedEnhancerModel(
        input_dim=input_dim, 
        num_classes=2,
        hidden_dims=OPTIMIZED_PARAMS['hidden_dims'],
        dropout_rate=OPTIMIZED_PARAMS['dropout_rate']
    ).to(device)
    
    global_model = AdvancedEnhancerModel(
        input_dim=input_dim, 
        num_classes=2,
        hidden_dims=OPTIMIZED_PARAMS['hidden_dims'],
        dropout_rate=OPTIMIZED_PARAMS['dropout_rate']
    ).to(device)

    print(f"=== {NUM_ROUNDS}ë¼ìš´ë“œ í•™ìŠµ ì‹œì‘ ===")
    
    # ë¼ìš´ë“œë³„ ì •í™•ë„ ì¶”ì 
    round_accuracies = []
    round_losses = []
    
    for r in range(NUM_ROUNDS):
        round_start_time = time.time()
        print(f"\nğŸš€ === ë¼ìš´ë“œ {r+1}/{NUM_ROUNDS} ì‹œì‘ ===")
        print(f"â° ì‹œì‘ ì‹œê°„: {time.strftime('%H:%M:%S')}")
        
        # 1ë‹¨ê³„: ê¸€ë¡œë²Œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        print(f"ğŸ“¥ 1ë‹¨ê³„: ì„œë²„ì—ì„œ ê¸€ë¡œë²Œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        try:
            state_dict = download_global_model()
            
            try:
                global_model.load_state_dict(state_dict)
                print(f"âœ… ê¸€ë¡œë²Œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ì„±ê³µ")
            except RuntimeError as e:
                if "size mismatch" in str(e):
                    print(f"âŒ ëª¨ë¸ ì°¨ì› ë¶ˆì¼ì¹˜: {e}")
                    print("ğŸ”„ ë¡œì»¬ ëª¨ë¸ ì´ˆê¸°í™”ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                    global_model.load_state_dict(client_model.state_dict())
                else:
                    raise e
        except Exception as e:
            print(f"âŒ ê¸€ë¡œë²Œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ë¡œì»¬ ëª¨ë¸ ì´ˆê¸°í™”ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            global_model.load_state_dict(client_model.state_dict())
        
        acc_before = evaluate_local_accuracy(client_model, train_loader, device)
        
        # 2ë‹¨ê³„: ë¡œì»¬ í•™ìŠµ ìˆ˜í–‰ (ìµœì í™”ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        print(f"ğŸ“ 2ë‹¨ê³„: ë¡œì»¬ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        training_start_time = time.time()
        
        try:
            updated_model, avg_loss, epochs, num_samples, accuracy = advanced_client_update(
                client_model, global_model, train_loader, nn.CrossEntropyLoss(), r, device, class_weights
            )
            print(f"âœ… ê³ ê¸‰ í•™ìŠµ í•¨ìˆ˜ ì‚¬ìš© ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ê³ ê¸‰ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return False
            
        training_end_time = time.time()
        training_duration = training_end_time - training_start_time
        acc_after = evaluate_local_accuracy(updated_model, train_loader, device)
        
        # í•™ìŠµëœ ëª¨ë¸ì„ í´ë¼ì´ì–¸íŠ¸ ëª¨ë¸ì— ë³µì‚¬
        client_model.load_state_dict(updated_model.state_dict())
        
        # 3ë‹¨ê³„: CKKS ì•”í˜¸í™”
        encryption_start_time = time.time()
        print(f"\nğŸ” 3ë‹¨ê³„: í´ë¼ì´ì–¸íŠ¸ ë°ì´í„° CKKS ì•”í˜¸í™”")
        state_dict = client_model.state_dict()
        print(f"ğŸ“¦ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {len(state_dict)}ê°œ ë ˆì´ì–´")
        
        # Tensor â†’ flat numpy vector
        flat_params = []
        for param_name, param_tensor in state_dict.items():
            flat_params.extend(param_tensor.cpu().numpy().flatten())
        
        flat_params = np.array(flat_params, dtype=np.float32)
        print(f"ğŸ“Š í‰ë©´í™”ëœ íŒŒë¼ë¯¸í„° í¬ê¸°: {flat_params.shape}")
        
        # CKKS ì•”í˜¸í™”
        encrypted_params = batch_encrypt(flat_params, batch_size=4)
        print(f"ğŸ”’ ì•”í˜¸í™”ëœ íŒŒë¼ë¯¸í„° ë°°ì¹˜ ìˆ˜: {len(encrypted_params[0])}")
        
        encryption_end_time = time.time()
        encryption_duration = encryption_end_time - encryption_start_time
        
        # 4ë‹¨ê³„: ì„œë²„ë¡œ ì „ì†¡
        upload_start_time = time.time()
        print(f"\nğŸ“¤ 4ë‹¨ê³„: ì•”í˜¸í™”ëœ íŒŒë¼ë¯¸í„° ì„œë²„ ì „ì†¡")
        
        try:
            # ì•”í˜¸í™”ëœ íŒŒë¼ë¯¸í„°ë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™” (ì„œë²„ UpdateRequest ëª¨ë¸ì— ë§ì¶¤)
            encrypted_data = {
                'client_id': CLIENT_ID,
                'round_id': r + 1,
                'c0_list': [c0.tolist() for c0 in encrypted_params[0]],
                'c1_list': [c1.tolist() for c1 in encrypted_params[1]],
                'original_size': len(flat_params),
                'num_samples': num_samples,
                'loss': float(avg_loss),
                'accuracy': float(accuracy)
            }
            
            response = requests.post(f"{SERVER_URL}/upload", json=encrypted_data, timeout=30)
            
            if response.status_code == 200:
                print(f"âœ… ì„œë²„ ì „ì†¡ ì„±ê³µ")
                server_response = response.json()
                print(f"ğŸ“Š ì„œë²„ ì‘ë‹µ: {server_response}")
            else:
                print(f"âŒ ì„œë²„ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ ì„œë²„ ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
        
        upload_end_time = time.time()
        upload_duration = upload_end_time - upload_start_time
        
        # ë¼ìš´ë“œ ì™„ë£Œ ìš”ì•½
        round_end_time = time.time()
        round_duration = round_end_time - round_start_time
        
        print(f"\n=== ë¼ìš´ë“œ {r+1} ì™„ë£Œ ===")
        print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {round_duration:.2f}ì´ˆ")
        print(f"  - í•™ìŠµ ì‹œê°„: {training_duration:.2f}ì´ˆ")
        print(f"  - ì•”í˜¸í™” ì‹œê°„: {encryption_duration:.2f}ì´ˆ")
        print(f"  - ì „ì†¡ ì‹œê°„: {upload_duration:.2f}ì´ˆ")
        print(f"ğŸ“ˆ ì •í™•ë„ ë³€í™”: {acc_before:.2f}% â†’ {acc_after:.2f}%")
        print(f"ğŸ¯ ìµœì¢… ì •í™•ë„: {accuracy:.2f}%")
        print(f"ğŸ“Š í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
        print(f"ğŸ“š í•™ìŠµ ì—í¬í¬: {epochs}")
        print(f"ğŸ“Š ìƒ˜í”Œ ìˆ˜: {num_samples}")
        
        # ë¼ìš´ë“œë³„ ì •í™•ë„ ë° ì†ì‹¤ ì €ì¥
        round_accuracies.append(accuracy)
        round_losses.append(avg_loss)
        
        # ì‹¤ì‹œê°„ ì •í™•ë„ ì°¨íŠ¸ ì¶œë ¥ (ë¼ìš´ë“œ 5ê°œë§ˆë‹¤)
        if (r + 1) % 5 == 0 or r == NUM_ROUNDS - 1:
            print(f"\nğŸ“ˆ === ë¼ìš´ë“œ {r+1}ê¹Œì§€ì˜ ì •í™•ë„ ì¶”ì´ ===")
            for i, acc in enumerate(round_accuracies):
                print(f"  ë¼ìš´ë“œ {i+1}: {acc:.2f}%")
            print(f"  í‰ê·  ì •í™•ë„: {np.mean(round_accuracies):.2f}%")
            print(f"  ìµœê³  ì •í™•ë„: {np.max(round_accuracies):.2f}%")
    
    # ìµœì¢… ëª¨ë¸ í‰ê°€
    print(f"\n=== ìµœì¢… ëª¨ë¸ í‰ê°€ ===")
    final_accuracy = evaluate_local_accuracy(client_model, test_loader, device)
    print(f"ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {final_accuracy:.2f}%")
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    print(f"\n=== ì˜ˆì¸¡ ìˆ˜í–‰ ===")
    try:
        # ì›ë³¸ ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_file)
        drop_cols = ['encounter_id', 'patient_nbr']
        df = df.drop(columns=drop_cols)
        df['readmitted'] = df['readmitted'].map(lambda x: 0 if x == 'NO' else 1)
        
        # ìˆ«ìí˜• íŠ¹ì„±ë§Œ ì„ íƒ
        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
        numeric_cols = [col for col in numeric_cols if col != 'readmitted']
        
        X = df[numeric_cols].values
        y = df['readmitted'].values
        
        # ìƒìˆ˜ íŠ¹ì„± ì œê±° í›„ íŠ¹ì„± ì„ íƒ (8ê°œë¡œ ê³ ì •)
        from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif
        
        # ìƒìˆ˜ íŠ¹ì„± ì œê±°
        variance_selector = VarianceThreshold(threshold=0.01)
        X_variance_filtered = variance_selector.fit_transform(X)
        non_constant_features = [numeric_cols[i] for i in variance_selector.get_support(indices=True)]
        
        # íŠ¹ì„± ì„ íƒ (8ê°œë¡œ ê³ ì •)
        k_features = min(8, X_variance_filtered.shape[1])
        selector = SelectKBest(score_func=f_classif, k=k_features)
        X_selected = selector.fit_transform(X_variance_filtered, y)
        selected_features_for_prediction = [non_constant_features[i] for i in selector.get_support(indices=True)]
        
        # ìŠ¤ì¼€ì¼ë§
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_selected)
        
        # ì˜ˆì¸¡ìš© ë°ì´í„°ì…‹ ìƒì„±
        from advanced_model import AdvancedDiabetesDataset
        prediction_dataset = AdvancedDiabetesDataset(X_scaled, np.zeros(len(X_scaled)))
        prediction_loader = DataLoader(prediction_dataset, batch_size=OPTIMIZED_PARAMS['batch_size'], shuffle=False)
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        probabilities, predictions, feature_importance = predict_diabetes_probability_with_explanation(
            client_model, prediction_loader, selected_features_for_prediction, device
        )
        
        # ê²°ê³¼ ë¶„ì„
        diabetes_count = np.sum(predictions == 1)
        normal_count = np.sum(predictions == 0)
        avg_probability = np.mean(probabilities)
        
        print(f"ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼:")
        print(f"  - ì´ ë°ì´í„°: {len(predictions):,}ê°œ")
        print(f"  - ë‹¹ë‡¨ë³‘ ì˜ˆì¸¡: {diabetes_count:,}ê°œ ({diabetes_count/len(predictions)*100:.1f}%)")
        print(f"  - ì •ìƒ ì˜ˆì¸¡: {normal_count:,}ê°œ ({normal_count/len(predictions)*100:.1f}%)")
        print(f"  - í‰ê·  ë‹¹ë‡¨ë³‘ í™•ë¥ : {avg_probability:.1%}")
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì¶œë ¥
        print(f"\nğŸ” íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 10ê°œ):")
        for i, (feature, importance) in enumerate(feature_importance[:10]):
            print(f"  {i+1:2d}. {feature}: {importance:.4f}")
        
        # ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ (ê·¸ë˜í”„ í¬í•¨)
        success = save_results_to_excel_with_graphs(
            df, probabilities, predictions, feature_importance, 
            'prediction_results_optimized.xlsx', round_accuracies, round_losses
        )
        
        if success:
            print(f"\nğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ê°€ 'prediction_results_optimized.xlsx'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"\nğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ê°€ 'prediction_results_optimized.csv'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print(f"\nğŸ‰ FedHybrid í´ë¼ì´ì–¸íŠ¸ (ìµœì í™” ë²„ì „) ì™„ë£Œ!")
    return True

if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)
